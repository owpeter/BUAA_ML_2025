# 机器学习数学基础：概率统计与矩阵知识梳理

## 概率统计理论基础

概率统计是机器学习中不可或缺的基石，它为理解数据、构建模型和评估结果提供了理论支撑。

### 随机试验与随机事件

* **随机试验**：指在一定条件下，结果不确定的观测或试验。
* **随机事件**：随机试验可能发生的结果，分为必然事件、不可能事件和随机事件。
* **事件关系与运算**：包括事件的包含 ($A\subset B$)、相等 ($A=B$)、积/交 ($A\cap B$)、互不相容 ($A\cap B=\Phi$)、和/并 ($A\cup B$)、对立 ($\overline{A}$)、差 ($A-B$)。事件间的运算满足交换律、结合律、分配律和对偶原则。

### 概率基本概念与重要公式

* **概率 (Probability)**：度量随机事件发生可能性大小的数值，满足非负性 ($0\le P(A)\le1$)、规范性 ($P(S)=1, P(\Phi)=0$)、有限可加性和互补性 ($P(\overline{A})=1-P(A)$)等性质。
* **联合概率 (Joint Probability)**：事件A和B共同发生的概率，记作 $P(A,B)$ 或 $P(A\cap B)$ 。
* **条件概率 (Conditional Probability)**：事件B已发生的条件下，事件A发生的概率，记作 $P(A|B)$ 。
* **独立事件 (Independent Events)**：事件A的发生不影响事件B发生的概率。
* **条件独立 (Conditional Independence)**：在给定事件C的条件下，事件A和B相互独立。
* **乘法公式 (Multiplication Theorem/Formula)**：$P(A,B)=P(A|B)\cdot P(B) =P(B|A)\cdot P(A)$ 。可推广到多个事件。
* **全概率公式 (Law of Total Probability)**：用于已知原因求结果的概率，$P(B)=\sum_{k=1}^{n}P(A_{k})P(B|A_{k})$ 。
* **贝叶斯公式 (Bayes' Theorem)**：用于已知结果求原因的概率，$P(B_{i}|A)=\frac{P(B_{i})P(A|B_{i})}{\sum_{j=1}^{n}P(B_{j})P(A|B_{j})}$ 。它连接了先验概率、似然概率和后验概率。

### 随机变量及其数字特征

* **随机变量**：将随机试验结果映射到实数上的函数。
* **概率分布函数 (CDF)**：描述随机变量取值不超过某个实数的概率，$F(x)=P(X\le x)$ 。具有单调不减、非负有界、右连续等性质。
* **离散型随机变量**：取值是有限或可数无限个的随机变量，其概率分布可用概率分布律表示。
* **连续型随机变量**：取值是不可数无限个的随机变量，其概率分布可用概率密度函数 (PDF) 描述。
* **期望 (Expectation)**：随机变量的概率平均值。
* **方差 (Variance)**：描述随机变量值偏离期望值的程度。
* **标准差 (Standard Deviation)**：方差的平方根。
* **矩**：包括原点矩和中心矩。
* **协方差 (Covariance)**：衡量两个随机变量线性相关程度。
* **相关系数 (Correlation Coefficient)**：标准化的协方差，$-1 \le \rho_{XY} \le 1$。
* **高斯分布 (Gaussian Distribution)**：机器学习中常见的概率分布，包括一维和多维形式。

### 概率密度函数估计

根据样本数据估计概率密度函数 $p(x)$。

* **参数化方法 (Parametric Methods)**：假设概率密度函数形式已知，估计未知参数。
    * **最大似然估计 (Maximum Likelihood Estimation, MLE)**：选择使似然函数（观测到样本集的概率密度）最大的参数值作为估计值。需要求解似然函数或对数似然函数的导数为零的方程。
    * 估计量的性质与评价标准：无偏性、渐近无偏性、有效性、一致性。
* **非参数化方法 (Nonparametric Methods)**：概率密度函数形式未知，直接依据样本估计总体分布。
    * **直方图方法**：将数据空间划分成小区域，统计落入各区域的样本数来估计概率密度。
    * **Parzen窗法 (Parzen Window Method)**：使用窗函数对样本进行加权，估计概率密度。
    * **k近邻法 (k-nearest Neighbor Method)**：固定落入区域内的样本数k，通过确定包含k个近邻的区域体积来估计概率密度。

## 矩阵基础概念 (课后拓展)

矩阵是处理多维数据和线性变换的重要工具。

### 矩阵的定义与特殊矩阵

* **矩阵**：由m行n列数组成的数表。
* **特殊矩阵**：方阵 (行数=列数)、行矩阵、列矩阵、零矩阵 (所有元素为零)、单位矩阵 (主对角线为1，其余为0)、对角矩阵 (非主对角线元素为0)、对称矩阵 ($a_{ij} = a_{ji}$)。
* **行列式与矩阵的区别**：行列式是一个数值，矩阵是一个数表。

### 矩阵的运算

* **加法**：同型矩阵对应元素相加。
* **数乘**：数与矩阵中每个元素相乘。
* **乘法**：第一个矩阵的列数等于第二个矩阵的行数时才能相乘。矩阵乘法不满足交换律。
* **幂**：方阵的多次相乘。
* **转置**：矩阵的行换成同序数的列得到的新矩阵。
* **逆矩阵**：对于方阵A，如果存在矩阵B使得 $AB=BA=E$，则B为A的逆矩阵，记作 $A^{-1}$。逆矩阵具有唯一性。
* **秩 (Rank)**：矩阵中不等于零的最高阶子式的阶数。
* **迹 (Trace)**：方阵主对角线元素之和。
* **求导**：向量对向量的求导得到Jacobian矩阵。

# 机器学习模型评估与选择：核心概念与方法

## 机器学习基础回顾

首先，回顾机器学习的定义：一个计算机程序如果在任务T上，用性能指标P衡量，随经验E的增加，性能得到提高，则称该程序从经验E中学习到了任务T。根据输出变量的类型和学习方式，机器学习主要分为监督学习（分类、回归）、无监督学习（聚类、降维）以及半监督学习。

* **监督学习**：数据拥有标签。
    * **分类**：输出是离散值，如手写数字识别。
    * **回归**：输出是连续值，如天气预测。
* **无监督学习**：数据没有标签。
    * **聚类**：将数据分组。
    * **降维**：减少数据维度。

分类和回归任务有不同的训练和测试过程，核心在于从带有标签的训练数据中学习规律，并对新的测试数据进行预测。

## 模型评估与选择的核心问题

模型的**误差**是预测输出与真实输出之间的差异。
* **训练误差**或**经验误差**：模型在训练集上的误差。
* **测试误差（Testing error）**：模型在新样本上的误差，反映模型的**泛化能力**。

模型评估与选择的目标是选择在未知数据上表现最好的模型，即最小化测试误差。这通常包括以下步骤：
1.  将数据集划分为训练集和测试集。
2.  在训练集上训练模型。
3.  在测试集上评估模型的泛化性能。
4.  基于评估结果进行模型选择。

## 数据集划分方法

为了客观评估模型的泛化能力，需要将数据集进行划分。常用的方法包括：

* **留出法（Hold-out）**：将数据集随机划分为训练集和测试集。通常采用2/3数据用于训练，1/3用于测试。优点是简单易行，缺点是受数据划分方式影响较大。
* **随机子抽样（Random Sub-sampling）**：多次重复留出法，取平均结果。可以减少单次划分带来的偏差，但每个样本被用于训练的次数不同。
* **k折交叉验证（k-fold Cross-validation）**：将数据集划分为k个大小相似的互斥子集。每次用k-1个子集训练，1个子集测试，重复k次。每个样本都被用于测试一次，用于训练k-1次。相比留出法和随机子抽样，评估结果更稳定可靠。
* **留一法（Leave-one-out）**：k折交叉验证的特殊情况，k等于样本总数。每次只留下一个样本做测试。评估结果比较准确，但计算量巨大，适用于数据集较小的情况。
* **自助法（Bootstrapping）**：通过有放回地从数据集中抽样，生成多个与原数据集大小相同的训练集。未被抽到的样本组成测试集。约有36.8%的样本不会被抽到。适用于数据集较小的情况，能够产生多个不同的训练集，对集成学习有利，但改变了数据集的初始分布，可能引入估计偏差。

不同的划分方法有各自的适用场景以及对方差和偏差的影响。

## 模型的性能度量

根据任务类型，有不同的性能度量指标。

### 回归任务

主要使用误差来衡量预测值与真实值之间的差距：
* **平均绝对误差（Mean Absolute Error, MAE）**：$E(f;D)=\frac{1}{n}\sum_{i=1}^{n}|f(x_{i})-y_{i}|$
* **均方误差（Mean Squared Error, MSE）**：$E(f;D)=\frac{1}{n}\sum_{i=1}^{n}(f(x_{i})-y_{i})^{2}$

### 分类任务

基本指标包括：
* **错误率**：分类错误的样本数占总样本数的比例。
* **准确率**：分类正确的样本数占总样本数的比例，1 - 错误率。
错误率和准确率直观且计算简单，但在数据类别不均衡时可能无法全面反映模型性能。

为了更详细地评估分类器性能，特别是处理类别不平衡问题时，引入以下概念和指标，通常基于**混淆矩阵（Confusion Matrix）**：

对于二分类问题（正例P，负例N）：

| 真实类别 \\ 预测结果 | 正例 P | 负例 N |
| :---------- | :----: | :----: |
| **正例 P** | 真正例 TP | 假负例 FN |
| **负例 N** | 假正例 FP | 真负例 TN |

> 这里对表中的四个概念进行一些解释：真正例指被分类器正确分类的正元组，真负例指被分类器正确分类的负元组，假正例指被分类器错标称正的负元组，加负例指被分类器错标为负的正元组

* **准确率（Accuracy）**：$accuracy = (TP+TN)/(P+N)$
* **错误率（ErrorRate）**：$ErrorRate = (FP+FN)/(P+N)$
* **查准率（Precision）**：预测为正例的样本中，真正例的比例。 $percision = TP/(TP+FP)$。衡量模型预测正例的准确性。
* **查全率（Recall）**或**敏感性（Sensitivity）**：所有真正例中，被模型正确识别为正例的比例 $recall = TP/P = TP/(TP+FN)$。衡量模型找出所有正例的能力。
* **真阳性率（True Positive Rate, TPR）**：等同于查全率。
* **真阴性率（True Negative Rate, TNR）**或**特异性（Specificity）**：所有真负例中，被模型正确识别为负例的比例 $TN/N = TN/(TN+FP)$。衡量模型找出所有负例的能力。
* **假阳性率（False Positive Rate, FPR）**：所有真负例中，被模型错误识别为正例的比例 $FP/N = FP/(FP+TN)$。

查准率和查全率往往存在矛盾，可以通过 **P-R曲线 Precision-Recall Curve**来可视化模型在不同阈值下的查准率和查全率表现。

综合查准率和查全率的指标：
* **F1度量**：查准率和查全率的调和平均，$F1 = 2 \times (Precision \times Recall) / (Precision + Recall)$。
* **F$_\beta$度量**：允许调整查全率对查准率的相对重要性，$F_\beta = (1+\beta^2) \times (Precision \times Recall) / (\beta^2 \times Precision + Recall)$。$\beta > 1$时更侧重查全率，$\beta < 1$时更侧重查准率。

其他重要的分类评估工具包括：
* **ROC曲线（Receiver Operating Characteristic Curve）**：以TPR为纵坐标，FPR为横坐标，反映模型在不同阈值下识别正例和负例的能力。 [cite: 50]
* **AUC（Area Under Curve）**：ROC曲线下的面积，用于衡量模型的整体性能，AUC值越大，模型性能越好。 [cite: 51]

### 代价敏感性能度量

在某些应用中，不同类型的错误可能导致不同的损失。**代价敏感性能度量**考虑了这一点，通过**代价矩阵（Cost Matrix）**定义不同误分类的代价，并计算**代价敏感错误率**。

## 贝叶斯决策理论：让机器做出更明智的判断

在机器学习领域，我们经常面临如何让机器根据已有数据做出最优决策的问题。贝叶斯决策理论正是解决此类问题的一大利器，它为我们提供了一个在不确定性条件下进行最优决策的数学框架。本文将带您梳理贝叶斯决策理论的核心概念，助您理解机器如何“思考”并做出判断。

### 一、概率论基础回顾：决策的基石

在深入贝叶斯决策理论之前，我们首先需要回顾几个关键的概率论概念，它们是理解贝叶斯决策的基石。

* **乘法公式**：用于计算多个事件同时发生的概率。简单来说，事件A和事件B同时发生的概率等于在事件B发生的条件下事件A发生的概率乘以事件B发生的概率，即 $P(A,B) = P(A|B) \cdot P(B)$。这个公式可以推广到多个事件的情况。
* **全概率公式**：当某个事件B的发生总是伴随着一系列互不相容的事件 $A_1, A_2, ..., A_n$ 之一发生时，事件B的概率可以通过对所有 $P(A_k)P(B|A_k)$ 求和得到。它描述了“知因求果”的过程，即在已知各种原因发生的概率及其导致结果发生的条件下，计算结果发生的总概率。
* **贝叶斯公式**：这是贝叶斯决策理论的核心。它恰好与全概率公式相反，描述了“知果求因”的过程。即在结果事件B已经发生的条件下，推断某个原因事件 $A_k$ 发生的条件概率。其基本形式为：
$$P(A_k|B) = \frac{P(A_k)P(B|A_k)}{\sum_{i=1}^{n}P(A_i)P(B|A_i)}$$

    公式中的 $P(A_k)$ 被称为**先验概率**（在观测到结果B之前对原因 $A_k$ 的判断），而 $P(A_k|B)$ 被称为**后验概率**（在观测到结果B之后对原因 $A_k$ 的修正判断）。贝叶斯公式告诉我们，对结果的任何观测都将增加我们对原因事件真实分布的认识。

### 二、贝叶斯决策理论：核心思想与应用

贝叶斯决策理论的目标是在各种可能的决策中选择期望损失最小的决策。在分类问题中，这意味着将样本划分到后验概率最大的那个类别。

#### 1. 基本概念

在讨论贝叶斯决策之前，我们先明确几个基本概念：

* **样本 (Sample)**：通常表示为一个d维向量 $x \in R^d$，代表我们观察到的一个具体实例。
* **类别/状态 (Class/State)**：用 $\omega_i$ 表示，代表样本可能属于的不同类别。
* **先验概率 (Prior Probability)**：$P(\omega_i)$，表示在没有任何其他信息的情况下，某个类别 $\omega_i$ 本身出现的概率。
* **类条件概率密度 (Class-conditional Probability Density)**：$p(x|\omega_i)$，表示在已知样本属于类别 $\omega_i$ 的条件下，观察到样本x的概率密度。
* **样本分布密度 (Sample Distribution Density)**：$p(x)$，表示观察到样本x的概率密度，可以通过全概率公式计算得到：$p(x) = \sum_{i} p(x|\omega_i)P(\omega_i)$。

#### 2. 最小错误率贝叶斯决策

这是贝叶斯决策中最常用的一种策略。其核心思想是：对于一个给定的样本x，我们计算它属于各个类别的后验概率 $P(\omega_i|x)$，然后选择具有最大后验概率的那个类别作为决策结果。这样做可以使得总体分类错误率最小。

根据贝叶斯公式，$P(\omega_i|x) = \frac{p(x|\omega_i)P(\omega_i)}{p(x)}$。由于对于同一个样本x，其 $p(x)$ 是相同的，因此比较后验概率 $P(\omega_i|x)$ 的大小，等价于比较 $p(x|\omega_i)P(\omega_i)$ 的大小。

所以，最小错误率贝叶斯决策的规则可以表示为：若 $p(x|\omega_i)P(\omega_i) > p(x|\omega_j)P(\omega_j)$ 对所有 $j \neq i$ 成立，则判决 $x \in \omega_i$。

#### 3. 最小风险贝叶斯决策

在某些场景下，不同的错分带来的损失是不同的。例如，在医疗诊断中，将病人误诊为健康（漏诊）和将健康人误诊为病人（误诊）所带来的风险可能不同。最小风险贝叶斯决策考虑了这种不同错分带来的损失。

它引入了一个**损失函数** $\lambda(\alpha_i|\omega_j)$，表示当真实类别是 $\omega_j$ 时，我们采取决策 $\alpha_i$ (即将样本判为类别i) 所带来的损失。那么，对于一个样本x，采取决策 $\alpha_i$ 的**条件风险** (Conditional Risk) 可以表示为所有可能真实类别的期望损失：$R(\alpha_i|x) = \sum_{j=1}^{c} \lambda(\alpha_i|\omega_j)P(\omega_j|x)$，其中c是类别总数。

最小风险贝叶斯决策的规则是：计算样本x采取每个可能决策 $\alpha_i$ 时的条件风险 $R(\alpha_i|x)$，然后选择使得条件风险最小的那个决策。

可以证明，最小错误率贝叶斯决策是最小风险贝叶斯决策在损失函数取特定值（例如，正确分类损失为0，错误分类损失为1）时的一个特例。

#### 4. 朴素贝叶斯决策

在实际应用中，直接估计类条件概率密度 $p(x|\omega_i)$ 往往非常困难，尤其是当样本x的维度很高时。朴素贝叶斯决策为了简化计算，做了一个很强的假设：**给定类别时，样本的各个特征之间是条件独立的。**

这意味着 $p(x|\omega_i) = p(x_1, x_2, ..., x_d|\omega_i) = \prod_{k=1}^{d} p(x_k|\omega_i)$，其中 $x_k$ 是样本x的第k个特征。

尽管这个独立性假设在现实中往往不成立，但朴素贝叶斯分类器在很多情况下仍然表现出惊人的良好性能，并且计算简单、易于实现。

#### 5. 贝叶斯估计

在前面的讨论中，我们假设先验概率 $P(\omega_i)$ 和类条件概率密度 $p(x|\omega_i)$ 是已知的。但在实际问题中，它们往往是未知的，需要从训练数据中进行估计。贝叶斯估计提供了一种估计这些概率参数的方法。与最大似然估计等方法不同，贝叶斯估计将待估计的参数也看作是随机变量，并为其引入先验分布，然后根据观测数据计算其后验分布。

### 总结

贝叶斯决策理论为我们提供了一个强大而灵活的框架，用于在不确定性下做出最优决策。从基本的概率回顾到最小错误率、最小风险以及朴素贝叶斯决策，再到参数的贝叶斯估计，这一理论贯穿了机器学习的诸多方面。理解贝叶斯决策的原理，不仅能帮助我们更好地应用相关的机器学习算法，也能启发我们对智能决策过程的深入思考。
